{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "l6pm_-bOloGC"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import nltk\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to construct markov model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RbO8zNURloGZ"
   },
   "outputs": [],
   "source": [
    "def construct_markov_model(model,n, data):\n",
    "    for item in data:\n",
    "        if  len(item)<n :\n",
    "            continue\n",
    "        add_ngram(model, n, item)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#size of n gram  need to be constructed\n",
    "ngram_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to create n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DNgonmWLloGQ"
   },
   "outputs": [],
   "source": [
    "def add_ngram(model, n, seq):\n",
    "    seq = list(seq[:]) + [None]\n",
    "    for i in range(len(seq)-n):\n",
    "        gram = tuple(seq[i:i+n])\n",
    "        next_item = seq[i+n]\n",
    "        if gram not in model:\n",
    "            model[gram] = []\n",
    "        model[gram].append(next_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to generate words for suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bdiz-d9sloGb"
   },
   "outputs": [],
   "source": [
    "def generator( model,n = 2, start=None, max=1):\n",
    "  try:\n",
    "      if start is None :\n",
    "        start = random.choice(key)\n",
    "      output = list(start)\n",
    "      for i in range(max):\n",
    "        start = tuple(output[-n:])\n",
    "        next = random.choice(model[start])\n",
    "        if next is None:\n",
    "          break\n",
    "        else:\n",
    "          output.append(next)\n",
    "      return \" \".join(output[n:])\n",
    "\n",
    "  except(KeyError):\n",
    "    return \" \".join(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "mPym7F7DloGy"
   },
   "outputs": [],
   "source": [
    "def sentence_split(data):\n",
    "\n",
    "    data = re.sub(r'unk|\\n \\n = [^=]*[^=] = \\n \\n|[\\/\\\\-]|[0-9,!?.\"\";()-\\,=<>@#%$*&~_{}:\\[\\]/\\'\\`]+', '', data)\n",
    "    sentences = data.split(\"\\n\")\n",
    "    new_data = []\n",
    "    for sentence in sentences:\n",
    "        new_data.append(sentence)\n",
    "    sentences = [s.strip() for s in new_data if len(s) > 1]\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6780Q1dGpAB9"
   },
   "outputs": [],
   "source": [
    "def tokenizer(sentences):\n",
    "    tokens = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        token = nltk.word_tokenize(sentence)\n",
    "        tokens.append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consturcting Markov Chain using wikipedia dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(i):\n",
    "    path = f\"//home//ticvictech//article//article{i}.txt\"\n",
    "    with open(path , \"r\") as f:\n",
    "        input_data = f.read()\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_model = {}\n",
    "for i in range(40000):\n",
    "    content = read_file(i)\n",
    "    content = sentence_split(content)\n",
    "    content = tokenizer(content)\n",
    "    markov_model =  construct_markov_model(markov_model,ngram_size,content)\n",
    "    content = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "fp = open(\"markov_recm\",\"wb\")\n",
    "pickle.dump(markov_model,fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing Markov Chain using chat datatset on top of wikipedia article dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OSdfVDfJxXrr"
   },
   "outputs": [],
   "source": [
    "fp1 = open(\"markov_recm\",\"rb\")\n",
    "recomender = pickle.load(fp1)\n",
    "fp1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/home/ticvictech/chat/Conversation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog_data = \"\"\n",
    "for data in df[\"question\"]:\n",
    "    dialog_data += data+\"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dialog_data = tokenizer(sentence_split(dialog_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = construct_markov_model(ngram_size,processed_dialog_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "f-NBWwEywGtY"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "fp = open(\"suggestion_model\",\"wb\")\n",
    "pickle.dump(model_2,fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating recommendation for given 2-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp2 = open(\"text_suggestion_model\",\"rb\")\n",
    "recomender = pickle.load(fp2)\n",
    "fp2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "opniYM33ZZ3Y",
    "outputId": "b481ffce-1646-4055-aa6a-cc0c781d5c16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 you solve\n",
      "0 you think this\n",
      "1 you say\n",
      "1 you intend on\n",
      "2 you think\n",
      "2 you laws to\n",
      "3 women see\n",
      "3 you know working\n",
      "4 you know\n",
      "4 women want forced\n",
      "5 you want\n",
      "5 you deal with\n",
      "6 we get\n",
      "6 you believe that\n",
      "7 media do\n",
      "7 we do have\n",
      "8 you have\n",
      "8 you sleep waking\n",
      "9 you not\n",
      "9 we pass from\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print( i,  generator( recomender,ngram_size,max=2,start = tuple(\"what do \".split())))\n",
    "    print( i,  generator(recomender,ngram_size, max=3,start = tuple(\"what do \".split())))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
